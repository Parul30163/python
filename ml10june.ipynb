{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaoKWPliXNPU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "hRMtainWX69J",
        "outputId": "efb03e2d-8362-4493-ee1c-3360eb1e50f3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-2d122a55-df4f-4eef-afdf-5fed0deb63ee\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-2d122a55-df4f-4eef-afdf-5fed0deb63ee\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving sherlock-holm.es_stories_plain-text_advs.txt to sherlock-holm.es_stories_plain-text_advs.txt\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "path_to_file = list(files.upload().keys())[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0PAKxZGHYZyc"
      },
      "outputs": [],
      "source": [
        "# read the text file\n",
        "with open(path_to_file, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ts5lP5qnYrsO"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fq3WMjIvYuHO"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayeAxJ_-Y6xp"
      },
      "outputs": [],
      "source": [
        "# now lets tockenize the text to create a sequence of words\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "total_words = len(tokenizer.word_index) + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jh9i4tmUabKk"
      },
      "source": [
        "In the above code, teh text is tockenized, which means it is divided into individual words or tocken. The 'Tokenizer' object is created, which will handel the tockenization process. The 'fit_on_text' method of the tockenizer is called, passing the 'text' as input. This method analyise the text and building a avoculabery of unique words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tg8sUNZ2aGFG"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in text.split('\\n'):\n",
        "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "    for i in range(1, len(token_list)):\n",
        "        n_gram_sequence = token_list[:i+1]\n",
        "        input_sequences.append(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s7C71p0AbasH"
      },
      "source": [
        "n the above code, yhe text data is splite into lines using te '\\n' character as delimiter. For each line in the text, the ,\n",
        "'text_to_sequence' method of the  tokenizer is used to  convert the line into a sequence of numerical tockens based on the previously created vocabulary. The resulting tocken list in th iterated over a for loop. For each iteration, a sub-sequence, or n-gram,  of tocken is extracted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrvrZTPVbZHc"
      },
      "outputs": [],
      "source": [
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d02PU26dcx9Z"
      },
      "source": [
        "In  the above code input sequences are padded to ensure all sequences have the same length. The variable 'max_sequence_len' is assigned the maximum length among all the input sequences. The 'pad_sequences'  function is used to add or truncate the input sequences to match thois maximum length.\n",
        "\n",
        "The 'pad_sequence' function takes the input_sequence list, sets the maximum length to 'max_sequence_len', and specifies that the padding should be added at the beginning of the each sequence using the 'padding=pre' argument. Finally, the input sequence are converted into a numpy array to facilate futher processing.\n",
        "\n",
        "Now lets split the sequence into input and output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7_s-pUckcuIs"
      },
      "outputs": [],
      "source": [
        "x = input_sequences[:, :-1]\n",
        "y = input_sequences[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBZ4-jdseNWA"
      },
      "source": [
        "In the above code, the input sequence are split into two arrays ,'x' and 'y', to cretae the input and output for training the next_word prediction model. the 'X' array is assigned the values of all rows in the 'input_sequence' array except for last column. It means the x contains all the tockens in each sequence except"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUUO8LYPeMCf"
      },
      "outputs": [],
      "source": [
        "y = np.array(tf.keras.utils.to_categorical(y, num_classes=total_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JAVTUXKe66B"
      },
      "source": [
        "In the above code we are converting the output array into a suitable formate for training a model, where each target word is represented as a binary vector.\n",
        "\n",
        "Now let's build a neural network architecture to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9-ihoBZe13V",
        "outputId": "80b5b5eb-9269-4111-e5f9-bd4b26db92d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 17, 100)           820000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 150)               150600    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 8200)              1238200   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2208800 (8.43 MB)\n",
            "Trainable params: 2208800 (8.43 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(LSTM(150))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "print(model.summary())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GUf0ScTqTGi"
      },
      "source": [
        "Now let us compile the training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxUYziK1e7dl",
        "outputId": "74786a49-9716-4d22-ea9f-c21d8fb1efe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 6.2407 - accuracy: 0.0762\n",
            "Epoch 2/100\n",
            "3010/3010 [==============================] - 87s 29ms/step - loss: 5.5097 - accuracy: 0.1241\n",
            "Epoch 3/100\n",
            "3010/3010 [==============================] - 90s 30ms/step - loss: 5.1221 - accuracy: 0.1471\n",
            "Epoch 4/100\n",
            "3010/3010 [==============================] - 90s 30ms/step - loss: 4.7932 - accuracy: 0.1647\n",
            "Epoch 5/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 4.4908 - accuracy: 0.1817\n",
            "Epoch 6/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 4.2047 - accuracy: 0.2029\n",
            "Epoch 7/100\n",
            "3010/3010 [==============================] - 90s 30ms/step - loss: 3.9326 - accuracy: 0.2275\n",
            "Epoch 8/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 3.6714 - accuracy: 0.2572\n",
            "Epoch 9/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 3.4263 - accuracy: 0.2905\n",
            "Epoch 10/100\n",
            "3010/3010 [==============================] - 90s 30ms/step - loss: 3.1989 - accuracy: 0.3266\n",
            "Epoch 11/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 2.9878 - accuracy: 0.3624\n",
            "Epoch 12/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 2.7937 - accuracy: 0.3973\n",
            "Epoch 13/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 2.6162 - accuracy: 0.4292\n",
            "Epoch 14/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 2.4512 - accuracy: 0.4625\n",
            "Epoch 15/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 2.3015 - accuracy: 0.4931\n",
            "Epoch 16/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 2.1633 - accuracy: 0.5200\n",
            "Epoch 17/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 2.0371 - accuracy: 0.5469\n",
            "Epoch 18/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 1.9201 - accuracy: 0.5718\n",
            "Epoch 19/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 1.8131 - accuracy: 0.5942\n",
            "Epoch 20/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 1.7145 - accuracy: 0.6144\n",
            "Epoch 21/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 1.6252 - accuracy: 0.6345\n",
            "Epoch 22/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 1.5406 - accuracy: 0.6509\n",
            "Epoch 23/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 1.4673 - accuracy: 0.6681\n",
            "Epoch 24/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 1.3969 - accuracy: 0.6826\n",
            "Epoch 25/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 1.3315 - accuracy: 0.6986\n",
            "Epoch 26/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 1.2722 - accuracy: 0.7128\n",
            "Epoch 27/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 1.2181 - accuracy: 0.7227\n",
            "Epoch 28/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 1.1676 - accuracy: 0.7353\n",
            "Epoch 29/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 1.1209 - accuracy: 0.7448\n",
            "Epoch 30/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 1.0774 - accuracy: 0.7553\n",
            "Epoch 31/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 1.0398 - accuracy: 0.7623\n",
            "Epoch 32/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 1.0015 - accuracy: 0.7703\n",
            "Epoch 33/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.9663 - accuracy: 0.7792\n",
            "Epoch 34/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.9359 - accuracy: 0.7849\n",
            "Epoch 35/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 0.9066 - accuracy: 0.7917\n",
            "Epoch 36/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.8805 - accuracy: 0.7972\n",
            "Epoch 37/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 0.8566 - accuracy: 0.8018\n",
            "Epoch 38/100\n",
            "3010/3010 [==============================] - 90s 30ms/step - loss: 0.8324 - accuracy: 0.8068\n",
            "Epoch 39/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.8122 - accuracy: 0.8115\n",
            "Epoch 40/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.7937 - accuracy: 0.8156\n",
            "Epoch 41/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 0.7758 - accuracy: 0.8190\n",
            "Epoch 42/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.7588 - accuracy: 0.8224\n",
            "Epoch 43/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 0.7413 - accuracy: 0.8260\n",
            "Epoch 44/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.7283 - accuracy: 0.8295\n",
            "Epoch 45/100\n",
            "3010/3010 [==============================] - 92s 30ms/step - loss: 0.7128 - accuracy: 0.8326\n",
            "Epoch 46/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 0.7031 - accuracy: 0.8330\n",
            "Epoch 47/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 0.6922 - accuracy: 0.8347\n",
            "Epoch 48/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6812 - accuracy: 0.8374\n",
            "Epoch 49/100\n",
            "3010/3010 [==============================] - 91s 30ms/step - loss: 0.6699 - accuracy: 0.8394\n",
            "Epoch 50/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6626 - accuracy: 0.8417\n",
            "Epoch 51/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.6536 - accuracy: 0.8424\n",
            "Epoch 52/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6435 - accuracy: 0.8451\n",
            "Epoch 53/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6365 - accuracy: 0.8459\n",
            "Epoch 54/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.6288 - accuracy: 0.8473\n",
            "Epoch 55/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6226 - accuracy: 0.8483\n",
            "Epoch 56/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6152 - accuracy: 0.8503\n",
            "Epoch 57/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6101 - accuracy: 0.8509\n",
            "Epoch 58/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.6036 - accuracy: 0.8520\n",
            "Epoch 59/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5968 - accuracy: 0.8545\n",
            "Epoch 60/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5959 - accuracy: 0.8539\n",
            "Epoch 61/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5891 - accuracy: 0.8547\n",
            "Epoch 62/100\n",
            "3010/3010 [==============================] - 96s 32ms/step - loss: 0.5844 - accuracy: 0.8556\n",
            "Epoch 63/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5803 - accuracy: 0.8560\n",
            "Epoch 64/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5728 - accuracy: 0.8582\n",
            "Epoch 65/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5733 - accuracy: 0.8568\n",
            "Epoch 66/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5673 - accuracy: 0.8582\n",
            "Epoch 67/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5640 - accuracy: 0.8594\n",
            "Epoch 68/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5614 - accuracy: 0.8590\n",
            "Epoch 69/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5624 - accuracy: 0.8582\n",
            "Epoch 70/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5601 - accuracy: 0.8588\n",
            "Epoch 71/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5556 - accuracy: 0.8592\n",
            "Epoch 72/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5550 - accuracy: 0.8597\n",
            "Epoch 73/100\n",
            "3010/3010 [==============================] - 92s 31ms/step - loss: 0.5506 - accuracy: 0.8604\n",
            "Epoch 74/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5468 - accuracy: 0.8609\n",
            "Epoch 75/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5495 - accuracy: 0.8592\n",
            "Epoch 76/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5439 - accuracy: 0.8613\n",
            "Epoch 77/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5427 - accuracy: 0.8625\n",
            "Epoch 78/100\n",
            "3010/3010 [==============================] - 97s 32ms/step - loss: 0.5419 - accuracy: 0.8619\n",
            "Epoch 79/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5407 - accuracy: 0.8608\n",
            "Epoch 80/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5374 - accuracy: 0.8619\n",
            "Epoch 81/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5340 - accuracy: 0.8623\n",
            "Epoch 82/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5329 - accuracy: 0.8634\n",
            "Epoch 83/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5316 - accuracy: 0.8634\n",
            "Epoch 84/100\n",
            "3010/3010 [==============================] - 98s 33ms/step - loss: 0.5358 - accuracy: 0.8615\n",
            "Epoch 85/100\n",
            "3010/3010 [==============================] - 99s 33ms/step - loss: 0.5374 - accuracy: 0.8614\n",
            "Epoch 86/100\n",
            "3010/3010 [==============================] - 99s 33ms/step - loss: 0.5299 - accuracy: 0.8634\n",
            "Epoch 87/100\n",
            "3010/3010 [==============================] - 98s 33ms/step - loss: 0.5325 - accuracy: 0.8620\n",
            "Epoch 88/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5270 - accuracy: 0.8630\n",
            "Epoch 89/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5268 - accuracy: 0.8629\n",
            "Epoch 90/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5289 - accuracy: 0.8631\n",
            "Epoch 91/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5270 - accuracy: 0.8624\n",
            "Epoch 92/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5290 - accuracy: 0.8619\n",
            "Epoch 93/100\n",
            "3010/3010 [==============================] - 95s 32ms/step - loss: 0.5161 - accuracy: 0.8663\n",
            "Epoch 94/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5213 - accuracy: 0.8633\n",
            "Epoch 95/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5201 - accuracy: 0.8643\n",
            "Epoch 96/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5200 - accuracy: 0.8634\n",
            "Epoch 97/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5142 - accuracy: 0.8659\n",
            "Epoch 98/100\n",
            "3010/3010 [==============================] - 95s 31ms/step - loss: 0.5167 - accuracy: 0.8640\n",
            "Epoch 99/100\n",
            "3010/3010 [==============================] - 93s 31ms/step - loss: 0.5209 - accuracy: 0.8634\n",
            "Epoch 100/100\n",
            "3010/3010 [==============================] - 94s 31ms/step - loss: 0.5198 - accuracy: 0.8632\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7bb9a05c0c70>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x, y, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0aXG8eTqW2r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-oMTS0C1qweL"
      },
      "source": [
        "In the above code, the model is being compiled and trained. The 'Compile' method configers the model for training. The 'loss' paramater is set to 'categorical_crossentropy', a commonly used loss function for multi class classification problems. The 'optimizer' paramater is set to 'adam', an optimization algorithm that adapts the learning rate during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxCetonPrnHP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1MtDR6Q8roCw"
      },
      "source": [
        "The 'metrics' paramater is set 'accuracy' to moniter the accuracy during training. After compiling the model, the 'fit' method is also called to train the mdoel on the input sequence \"X\" and the corresponding output 'y'. The 'epochs' parameter specifies the number of times the training process will iterate over hte entire dataset. The 'verbose' paramater is set to '1' to display the training process.\n",
        "\n",
        "The above code will take more than an hour to execute. Once this cod eis executed, here]'s how we can generate the next word predicting using our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpTyW3ddskK5",
        "outputId": "4ad9062e-5dca-4d3e-8d26-47c056a70ab0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "I am afraid that you\n"
          ]
        }
      ],
      "source": [
        "# seed_ text = \"I will leave if they \"\n",
        "send_text = \"I am\"\n",
        "next_words = 3\n",
        "\n",
        "for _ in range(next_words):\n",
        "    token_list = tokenizer.texts_to_sequences([send_text])[0]\n",
        "    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "    output_word = \"\"\n",
        "    for word, index in tokenizer.word_index.items():\n",
        "        if index == predicted:\n",
        "            output_word = word\n",
        "            break\n",
        "    send_text += \" \" + output_word\n",
        "print(send_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alaElMsDY2xi"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
