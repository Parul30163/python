{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqlO9d3/35JdP9pOw+8I8T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Parul30163/python/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "10GtEHxaiWpc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "df=pd.read_csv(\"C:\\\\Users\\\\PARUL\\\\Downloads\\\\IMDB Dataset.csv\")\n",
        "\n",
        "df.head()\n",
        "\n",
        "df['review'][3]\n",
        "\n",
        "# step 1 lowercasing\n",
        "\n",
        "df['review'][3].lower()\n",
        "\n",
        "# if i want to convert  all movis review then\n",
        "\n",
        "df['review']=df['review'].str.lower()\n",
        "\n",
        "df\n",
        "\n",
        "# step 2 Remove HTML tags\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import re\n",
        "def remove_html_tags(text):\n",
        "    pattern =re.complie('<.*?>')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "text =\"<html><body><p> Movie1 </p><p> Click here to <a href ='http'\"\n",
        "\n",
        "remove_html_tags(text)\n",
        "\n",
        "df['review']= df['review'].apply(remove_html_tags)\n",
        "\n",
        "# if we want to remove url\n",
        "\n",
        "def remove_url(text):\n",
        "    pattern =re.complie(r'https?://\\S+|www\\.\\S+')\n",
        "    return pattern.sub(r'', text)\n",
        "\n",
        "text1='Check out my notevook https://\\S+|www\\.\\S+'\n",
        "\n",
        "remove_url(text1)\n",
        "\n",
        "# step 4 remove punctuation\n",
        "\n",
        "import string\n",
        "import time\n",
        "\n",
        "string.punctuation\n",
        "\n",
        "exclude =string.punctuation\n",
        "\n",
        "def remove_punc(text):\n",
        "    for char in exclude:\n",
        "        text =text.replace(char, '')\n",
        "    return text\n",
        "\n",
        "text1='string.With.Punctuation?'\n",
        "\n",
        "remove_punc(text1)\n",
        "\n",
        "df['review']= df['review'].apply(remove_punc)\n",
        "\n",
        "start =time.time()\n",
        "print(remove_punc(text))\n",
        "time1=time.time()-start\n",
        "\n",
        "df=pd.read_csv(\"C:\\\\Users\\\\PARUL\\\\Downloads\\\\twitter_sentiment_analysis.csv\")\n",
        "\n",
        "df\n",
        "\n",
        "import string\n",
        "exclude= string.punctuation\n",
        "\n",
        "def removepinc(text):\n",
        "    return text.translate(str.maketrans('','',exclude))\n",
        "\n",
        "\n",
        "df['tweet']=df['tweet'].apply(remove_punc)\n",
        "\n",
        "df\n",
        "\n",
        "# step=5 chatword treatment\n",
        "\n",
        "chat_words= {'u2':'you too','BBL':'Be Back Later','GAL':'Get A life'}\n",
        "\n",
        "chat_words\n",
        "\n",
        "def chat_conversation(text):\n",
        "    new_text=[]\n",
        "    for w in text.split():\n",
        "        if w.upper( )in chat_words :\n",
        "            new_text.append(chat_words[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "\n",
        "    return\"\".join(new_text)\n",
        "\n",
        "chat_conversation('GAL')\n",
        "\n",
        "# step-6 spelling correction\n",
        "\n",
        "!pip install TextBlob\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "incorrect_text =\"ceertain conditionas seveal ggenrtations aree modified in te saame maner\"\n",
        "\n",
        "textblb=TextBlob(incorrect_text)\n",
        "\n",
        "textblb.correct().string\n",
        "\n",
        "# Step-7 Removing stepwords\n",
        "\n",
        "#natural language tool kit >> nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download (\"stopwords\")\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "STOPWORDS =set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "# STOPWORDS\n",
        "\n",
        "#manually work\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    new_text =[]\n",
        "\n",
        "    for word in text.split():\n",
        "\n",
        "        if word in set(stopwords.words('english')):\n",
        "            new_text.append('')\n",
        "\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "\n",
        "\n",
        "    x=new_text[:]\n",
        "    new_text.clear()\n",
        "    return\"\".join(x)\n",
        "\n",
        "remove_stopwords('probably my all-time fav movie , a story of selflessness , sacfirirsn and dedication to a nobel xause')\n",
        "\n",
        "# tokenixation\n",
        "\n",
        "#1.prefix -character(s) at the begining =$(\" .)\n",
        "\n",
        "#2. suffix = character(s) at the end =km).!\"\n",
        "\n",
        "#3. infix - character(s) in between = --/...\n",
        "\n",
        "# 4. Exception - Special case rule to split a string into several tokens or\n",
        "#prevent a token from being split  when punctuation rules are applies ,=\n",
        "\n",
        "\n",
        "# word tokenizer\n",
        "sent1=\"i am going to delhi\"\n",
        "sent1.split()\n",
        "\n",
        "#sentence tokenizier\n",
        "sent2 =\"i am going to delhi. i will stay there for 3 days\"\n",
        "sent2.split('.')\n",
        "\n",
        "# problems with split function\n",
        "sent3= \"i m going to delhi!\"\n",
        "sent3.split()\n",
        "\n",
        "#in sent3 problem , there is seeing ! is included with delhi\n",
        "\n",
        "import re\n",
        "\n",
        "sent3=\"i am going to delhi\"\n",
        "tokens =re.findall(\"[\\w']+\" ,sent3)\n",
        "sent3\n",
        "\n",
        "text=''' xxyzesb is simpley dummy tesy printinf industry?\n",
        "jygvbh,k.'''\n",
        "\n",
        "# NLTK\n",
        "\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "sent1=\"i an going to delhi!\"\n",
        "word_tokenize(sent1)\n",
        "\n",
        "sent2=\" i have a Ph.D. in A.I.\"\n",
        "\n",
        "word_tokenize(sent2)\n",
        "\n",
        "# Stemming\n",
        "\n",
        "#1. Inflection ==> \"In grammar, inflection is the modification of a word\n",
        "\n",
        "# to express different grametical categories such as tense, case, voice, aspect, person, #humber, gender and mood.\"\n",
        "#number ,gender and mood.\n",
        "\n",
        "#2. Stemming ==> Stemming is the process of reducing inflection in words to their root forms\n",
        "\n",
        "# such as mapping a group of words to the same stem even if the stem itself is not valid\n",
        "\n",
        "#word in the Language\n",
        "\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "ps= PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([ps.stem(word)for word in text.split()])\n",
        "\n",
        "sample=\" walk walks walking walked\"\n",
        "stem_words(sample)\n",
        "\n",
        "# stemming mapping a group of words to the same even if the stem itself is not calid word in language\n",
        "text ='probably my alltime fav story of selflessness sacrifice and dedication'\n",
        "print(text)\n",
        "\n",
        "stem_words(text)\n",
        "\n",
        "#Lemmatization unlike Stemming, reduces the inflected words properly ensuring that\n",
        "#the root word belongs το the Language.\n",
        "\n",
        "# In Lemitization root word is called Lemma\n",
        "\n",
        "#ALemma (plural Lemmos or Lemmato) is the canonical ical form dictionary form, or citation form\n",
        "\n",
        "# of a set of words\n",
        "\n",
        "# Stemming works on behalf algorithm so it is fast and  fast and Lemmotization is using wordnet package\n",
        "#so its slow.\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "wordnet_lemmatizer =WordNetLemmatizer()\n",
        "\n",
        "sentence =' he was running and eating at te same time . he has bad habbit of swimming after playing long hours in the sun'\n",
        "punctuation ='?:!.,;'\n",
        "sentence_words=nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    print('{0:20}{1:20}'.format(word,wordnet_lemmatizer.lemmatize(word)))\n",
        "\n"
      ]
    }
  ]
}